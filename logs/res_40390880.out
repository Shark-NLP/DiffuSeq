diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28
torchrun --nproc_per_node=1 --master_port=12341 /scratch/ad6489/thesis/DiffuSeq/sample_seq2seq.py --model_path diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/ema_0.9999_050000.pt --step 2000 --batch_size 50 --seed2 108 --split test --out_dir generation_outputs --top_p -1 
Namespace(model_path='diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/ema_0.9999_050000.pt', step=2000, out_dir='generation_outputs', top_p=-1, lr=0.0001, batch_size=50, microbatch=64, learning_steps=320000, log_interval=20, save_interval=40000, eval_interval=1000, ema_rate='0.9999', resume_checkpoint='none', schedule_sampler='lossaware', diffusion_steps=2000, noise_schedule='sqrt', timestep_respacing='', vocab='bert', use_plm_init='no', vocab_size=0, config_name='huggingface-config', notes='folder-notes', data_dir='data-dir', dataset='dataset-name', checkpoint_path='checkpoint-path', seq_len=128, hidden_t_dim=128, hidden_dim=128, dropout=0.1, use_fp16=False, fp16_scale_growth=0.001, seed=102, gradient_clipping=-1.0, weight_decay=0.0, learn_sigma=False, use_kl=False, predict_xstart=True, rescale_timesteps=True, rescale_learned_sigmas=False, sigma_small=False, emb_scale_factor=1.0, split='test', clamp_step=0, seed2=108, clip_denoised=False)
Logging to /state/partition1/job-40390880/openai-2023-11-27-03-35-13-635905
diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/training_args.json
### Creating model and diffusion...
### The parameter count is 91225274
### Sampling...on test
############################## 
Loading text data...
############################## 
Loading dataset qqp from /scratch/ad6489/thesis/DiffuSeq/datasets/QQP...
### Loading form the TEST set...
### Data samples...
 ['Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?', 'How can I be a good geologist?'] ["I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?", 'What should I do to be a great geologist?']
RAM used: 1358.04 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 2500
})
RAM used: 1359.95 MB
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 2500
})
### tokenized_datasets...example [101, 28625, 6483, 1024, 1045, 2572, 1037, 6178, 7277, 9691, 3103, 6178, 4231, 1998, 6178, 4803, 1012, 1012, 1012, 2054, 2515, 2008, 2360, 2055, 2033, 1029, 102]
RAM used: 1365.67 MB
RAM used: 1369.20 MB
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 2500
}) padded dataset
RAM used: 1382.46 MB
RAM used: 1382.46 MB
### End of reading iteration...
### Total takes 19083.54s .....
### Written the decoded output to generation_outputs/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/ema_0.9999_050000.pt.samples/seed108_step0.json
torchrun --nproc_per_node=1 --master_port=12341 /scratch/ad6489/thesis/DiffuSeq/sample_seq2seq.py --model_path diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/ema_0.9999_040000.pt --step 2000 --batch_size 50 --seed2 108 --split test --out_dir generation_outputs --top_p -1 
Namespace(model_path='diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/ema_0.9999_040000.pt', step=2000, out_dir='generation_outputs', top_p=-1, lr=0.0001, batch_size=50, microbatch=64, learning_steps=320000, log_interval=20, save_interval=40000, eval_interval=1000, ema_rate='0.9999', resume_checkpoint='none', schedule_sampler='lossaware', diffusion_steps=2000, noise_schedule='sqrt', timestep_respacing='', vocab='bert', use_plm_init='no', vocab_size=0, config_name='huggingface-config', notes='folder-notes', data_dir='data-dir', dataset='dataset-name', checkpoint_path='checkpoint-path', seq_len=128, hidden_t_dim=128, hidden_dim=128, dropout=0.1, use_fp16=False, fp16_scale_growth=0.001, seed=102, gradient_clipping=-1.0, weight_decay=0.0, learn_sigma=False, use_kl=False, predict_xstart=True, rescale_timesteps=True, rescale_learned_sigmas=False, sigma_small=False, emb_scale_factor=1.0, split='test', clamp_step=0, seed2=108, clip_denoised=False)
Logging to /state/partition1/job-40390880/openai-2023-11-27-08-53-27-355664
diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/training_args.json
### Creating model and diffusion...
### The parameter count is 91225274
### Sampling...on test
############################## 
Loading text data...
############################## 
Loading dataset qqp from /scratch/ad6489/thesis/DiffuSeq/datasets/QQP...
### Loading form the TEST set...
### Data samples...
 ['Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?', 'How can I be a good geologist?'] ["I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?", 'What should I do to be a great geologist?']
RAM used: 1358.80 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 2500
})
RAM used: 1360.71 MB
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 2500
})
### tokenized_datasets...example [101, 28625, 6483, 1024, 1045, 2572, 1037, 6178, 7277, 9691, 3103, 6178, 4231, 1998, 6178, 4803, 1012, 1012, 1012, 2054, 2515, 2008, 2360, 2055, 2033, 1029, 102]
RAM used: 1366.31 MB
RAM used: 1370.36 MB
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 2500
}) padded dataset
RAM used: 1383.64 MB
RAM used: 1383.64 MB
### End of reading iteration...
### Total takes 18996.96s .....
### Written the decoded output to generation_outputs/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/ema_0.9999_040000.pt.samples/seed108_step0.json
torchrun --nproc_per_node=1 --master_port=12341 /scratch/ad6489/thesis/DiffuSeq/sample_seq2seq.py --model_path diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/ema_0.9999_030000.pt --step 2000 --batch_size 50 --seed2 108 --split test --out_dir generation_outputs --top_p -1 
Namespace(model_path='diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/ema_0.9999_030000.pt', step=2000, out_dir='generation_outputs', top_p=-1, lr=0.0001, batch_size=50, microbatch=64, learning_steps=320000, log_interval=20, save_interval=40000, eval_interval=1000, ema_rate='0.9999', resume_checkpoint='none', schedule_sampler='lossaware', diffusion_steps=2000, noise_schedule='sqrt', timestep_respacing='', vocab='bert', use_plm_init='no', vocab_size=0, config_name='huggingface-config', notes='folder-notes', data_dir='data-dir', dataset='dataset-name', checkpoint_path='checkpoint-path', seq_len=128, hidden_t_dim=128, hidden_dim=128, dropout=0.1, use_fp16=False, fp16_scale_growth=0.001, seed=102, gradient_clipping=-1.0, weight_decay=0.0, learn_sigma=False, use_kl=False, predict_xstart=True, rescale_timesteps=True, rescale_learned_sigmas=False, sigma_small=False, emb_scale_factor=1.0, split='test', clamp_step=0, seed2=108, clip_denoised=False)
Logging to /state/partition1/job-40390880/openai-2023-11-27-14-10-14-259583
diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/training_args.json
### Creating model and diffusion...
### The parameter count is 91225274
### Sampling...on test
############################## 
Loading text data...
############################## 
Loading dataset qqp from /scratch/ad6489/thesis/DiffuSeq/datasets/QQP...
### Loading form the TEST set...
### Data samples...
 ['Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?', 'How can I be a good geologist?'] ["I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?", 'What should I do to be a great geologist?']
RAM used: 1358.40 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 2500
})
RAM used: 1360.32 MB
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 2500
})
### tokenized_datasets...example [101, 28625, 6483, 1024, 1045, 2572, 1037, 6178, 7277, 9691, 3103, 6178, 4231, 1998, 6178, 4803, 1012, 1012, 1012, 2054, 2515, 2008, 2360, 2055, 2033, 1029, 102]
RAM used: 1365.74 MB
RAM used: 1369.42 MB
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 2500
}) padded dataset
RAM used: 1382.72 MB
RAM used: 1382.72 MB
### End of reading iteration...
### Total takes 19034.65s .....
### Written the decoded output to generation_outputs/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/ema_0.9999_030000.pt.samples/seed108_step0.json
torchrun --nproc_per_node=1 --master_port=12341 /scratch/ad6489/thesis/DiffuSeq/sample_seq2seq.py --model_path diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/ema_0.9999_020000.pt --step 2000 --batch_size 50 --seed2 108 --split test --out_dir generation_outputs --top_p -1 
Namespace(model_path='diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/ema_0.9999_020000.pt', step=2000, out_dir='generation_outputs', top_p=-1, lr=0.0001, batch_size=50, microbatch=64, learning_steps=320000, log_interval=20, save_interval=40000, eval_interval=1000, ema_rate='0.9999', resume_checkpoint='none', schedule_sampler='lossaware', diffusion_steps=2000, noise_schedule='sqrt', timestep_respacing='', vocab='bert', use_plm_init='no', vocab_size=0, config_name='huggingface-config', notes='folder-notes', data_dir='data-dir', dataset='dataset-name', checkpoint_path='checkpoint-path', seq_len=128, hidden_t_dim=128, hidden_dim=128, dropout=0.1, use_fp16=False, fp16_scale_growth=0.001, seed=102, gradient_clipping=-1.0, weight_decay=0.0, learn_sigma=False, use_kl=False, predict_xstart=True, rescale_timesteps=True, rescale_learned_sigmas=False, sigma_small=False, emb_scale_factor=1.0, split='test', clamp_step=0, seed2=108, clip_denoised=False)
Logging to /state/partition1/job-40390880/openai-2023-11-27-19-27-41-064676
diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/training_args.json
### Creating model and diffusion...
### The parameter count is 91225274
### Sampling...on test
############################## 
Loading text data...
############################## 
Loading dataset qqp from /scratch/ad6489/thesis/DiffuSeq/datasets/QQP...
### Loading form the TEST set...
### Data samples...
 ['Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?', 'How can I be a good geologist?'] ["I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?", 'What should I do to be a great geologist?']
RAM used: 1358.83 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 2500
})
RAM used: 1360.95 MB
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 2500
})
### tokenized_datasets...example [101, 28625, 6483, 1024, 1045, 2572, 1037, 6178, 7277, 9691, 3103, 6178, 4231, 1998, 6178, 4803, 1012, 1012, 1012, 2054, 2515, 2008, 2360, 2055, 2033, 1029, 102]
RAM used: 1366.42 MB
RAM used: 1370.07 MB
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 2500
}) padded dataset
RAM used: 1383.34 MB
RAM used: 1383.34 MB
### End of reading iteration...
### Total takes 19042.09s .....
### Written the decoded output to generation_outputs/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/ema_0.9999_020000.pt.samples/seed108_step0.json
torchrun --nproc_per_node=1 --master_port=12341 /scratch/ad6489/thesis/DiffuSeq/sample_seq2seq.py --model_path diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/ema_0.9999_010000.pt --step 2000 --batch_size 50 --seed2 108 --split test --out_dir generation_outputs --top_p -1 
Namespace(model_path='diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/ema_0.9999_010000.pt', step=2000, out_dir='generation_outputs', top_p=-1, lr=0.0001, batch_size=50, microbatch=64, learning_steps=320000, log_interval=20, save_interval=40000, eval_interval=1000, ema_rate='0.9999', resume_checkpoint='none', schedule_sampler='lossaware', diffusion_steps=2000, noise_schedule='sqrt', timestep_respacing='', vocab='bert', use_plm_init='no', vocab_size=0, config_name='huggingface-config', notes='folder-notes', data_dir='data-dir', dataset='dataset-name', checkpoint_path='checkpoint-path', seq_len=128, hidden_t_dim=128, hidden_dim=128, dropout=0.1, use_fp16=False, fp16_scale_growth=0.001, seed=102, gradient_clipping=-1.0, weight_decay=0.0, learn_sigma=False, use_kl=False, predict_xstart=True, rescale_timesteps=True, rescale_learned_sigmas=False, sigma_small=False, emb_scale_factor=1.0, split='test', clamp_step=0, seed2=108, clip_denoised=False)
Logging to /state/partition1/job-40390880/openai-2023-11-28-00-45-13-853642
diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20231116-03:15:28/training_args.json
### Creating model and diffusion...
### The parameter count is 91225274
### Sampling...on test
############################## 
Loading text data...
############################## 
Loading dataset qqp from /scratch/ad6489/thesis/DiffuSeq/datasets/QQP...
### Loading form the TEST set...
### Data samples...
 ['Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?', 'How can I be a good geologist?'] ["I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?", 'What should I do to be a great geologist?']
RAM used: 1358.65 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 2500
})
RAM used: 1360.71 MB
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 2500
})
### tokenized_datasets...example [101, 28625, 6483, 1024, 1045, 2572, 1037, 6178, 7277, 9691, 3103, 6178, 4231, 1998, 6178, 4803, 1012, 1012, 1012, 2054, 2515, 2008, 2360, 2055, 2033, 1029, 102]
RAM used: 1366.37 MB
RAM used: 1370.70 MB
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 2500
}) padded dataset
RAM used: 1383.77 MB
RAM used: 1383.77 MB
### End of reading iteration...
